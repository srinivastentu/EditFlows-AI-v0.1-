{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0e5661-67d1-4641-8d19-42b1d1346d73",
   "metadata": {},
   "source": [
    "### PROJECT: AI IN FILM EDITING\n",
    "\n",
    "# PROJECT TITLE: EDITFLOWS AI\n",
    "\n",
    "## DEVELOPED BY: SRINIVAS TENTU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649d484-04d7-437b-96a8-8524eb8ac55a",
   "metadata": {},
   "source": [
    "### INTRODUCTION:\n",
    "\n",
    "Hello, I‚Äôm Srinivas Tentu.\n",
    "This document presents my capstone project for the **Minor in AI program at IIT Ropar - Masai**.\n",
    "\n",
    "The project is called **EditFlows AI**, and it explores how artificial intelligence (AI) can assist film editing, specifically at the most time-consuming and cognitively demanding stage ‚Äî the first cut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27490b8-4af6-4332-ac65-9c1946841759",
   "metadata": {},
   "source": [
    "### PROBLEM DEFINITION:\n",
    "\n",
    "Film editing is not slow because of rendering or software limitations. It is slow because of editorial decision-making. Creating a first cut of a short film, documentary, or episode often takes days or weeks.\n",
    "\n",
    "The editor must understand the story, evaluate raw footage, test multiple narrative possibilities, and only then move into refinement.\n",
    "\n",
    "Modern video editing tools focus on execution (cutting, trimming, effects) but lack editorial intelligence. Editors must manually understand story intent, scene structure, dialogue logic, spatial continuity, and emotional flow before making any meaningful edit. This cognitive load is time-consuming, error-prone, and difficult to scale‚Äîespecially for long-form or multi-take footage.\n",
    "\n",
    "Current AI editing solutions generate cuts without exposing their editorial reasoning, resulting in incoherent edits, continuity breaks, repeated dialogue, and a lack of narrative control. Editors cannot inspect, correct, or guide the AI‚Äôs understanding before edits are generated, making these tools unsuitable for real editorial workflows.\n",
    "\n",
    "There is a lack of systems that treat editorial understanding as a formal, inspectable, and correctable intermediate representation before automated editing.EditFlows AI addresses this gap by treating editorial understanding as a first-class, inspectable artifact, thereby bridging the gap between human editorial judgment and AI automation.\n",
    "\n",
    "### OBJECTIVE:\n",
    "The objective of EditFlows AI is to develop an AI-assisted video editing system that formalizes editorial understanding as an explicit intermediate layer between raw footage and final edits.\n",
    "\n",
    "### Specifically, the system aims to:\n",
    "* Extract and structure editorial intelligence across story, visual, audio, and continuity dimensions.\n",
    "* Allow human editors to inspect and optionally correct AI-generated editorial representations before edit generation.\n",
    "* Produce narrative-coherent first cuts driven by validated editorial logic rather than heuristic clip assembly.\n",
    "* Support iterative human‚ÄìAI collaboration in editing workflows.\n",
    "* Export non-destructive edit instructions (XML) compatible with professional video editing software.\n",
    "\n",
    "This project aims to demonstrate that exposing and validating editorial reasoning significantly improves the quality, controllability, and reliability of AI-assisted video editing systems.\n",
    "\n",
    "The broader goal is to establish a foundation for an AI-native film editing workflow, where AI augments editorial thinking instead of replacing it ‚Äî ultimately evolving toward a scalable AI Film Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4187968-74a1-467c-9294-b6cb3ccbfced",
   "metadata": {},
   "source": [
    "### DATA UNDERSTANDING & PREPARATION:\n",
    "\n",
    "This project does not involve training any machine learning or deep learning models from scratch. Instead, the focus is on leveraging existing Large Language Models (LLMs) and specialized AI tools as reasoning engines within a structured film-editing system.\n",
    "\n",
    "### Dataset Source:\n",
    "The primary data consists of locally collected raw video footage, captured specifically for testing and validating the EditFlows AI system. No public datasets were used for training. External AI models were accessed via APIs strictly for inference and analysis.\n",
    "\n",
    "### Data Loading and Exploration:\n",
    "Video files were loaded from the local file system and analyzed to extract clip-level metadata, audio tracks, and frame samples. Initial exploration focused on understanding clip durations, dialogue presence, spatial context, and performance characteristics relevant to editorial decision-making.\n",
    "\n",
    "### Data Preparation and Feature Extraction:\n",
    "Rather than feature engineering for model training, the system performs editorial feature extraction, including:\n",
    "* Audio transcription and dialogue classification\n",
    "* Detection of production audio versus in-scene dialogue\n",
    "* Scene and shot grouping\n",
    "* Spatial and continuity inference\n",
    "* Emotional and narrative state identification\n",
    "\n",
    "These features are stored as structured editorial artifacts used to guide automated edit generation.\n",
    "\n",
    "### Handling Noise and Ambiguity:\n",
    "Background noise, production audio (e.g., director cues), and ambiguous visual context are explicitly identified and categorized instead of being removed. This preserves real-world filmmaking conditions and allows the system to make informed editorial decisions rather than assuming clean input data.\n",
    "\n",
    "### Key Clarification:\n",
    "EditFlows AI is a system-level AI application, not a model-training project. Its contribution lies in orchestrating existing AI capabilities into a coherent, editor-centric workflow for intelligent film editing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34ed4f-48d3-4e77-b818-bf4fc034167f",
   "metadata": {},
   "source": [
    "### MODEL / SYSTEM DESIGN:\n",
    "\n",
    "### AI Technique Used\n",
    "EditFlows AI follows a hybrid AI system design that combines:\n",
    "* **Large Language Models (LLMs)** for narrative reasoning, editorial logic, and semantic understanding\n",
    "* **Speech-to-text models** for dialogue transcription and audio classification\n",
    "* **Vision-capable models** for high-level visual scene understanding\n",
    "* **Rule-based editorial logic** to enforce film grammar, continuity, and deterministic editing constraints\n",
    "\n",
    "No custom ML/DL models were trained; all AI models are used strictly for inference via APIs.\n",
    "\n",
    "### System Architecture and Pipeline:\n",
    "The system is designed as a modular, multi-stage editorial pipeline:\n",
    "Application UI clearly demonstrates these stages as follows:\n",
    "    PROJECT -> MEDIA -> AI UNDERSTANDING -> GENERATE EDITS -> EXPORT XML\n",
    "\n",
    "1. Ingestion Stage:\n",
    "   Raw video footage is loaded locally, and metadata such as duration, file paths, and audio tracks is extracted.\n",
    "\n",
    "2. Analysis & Understanding Stage:\n",
    "   AI models generate structured editorial artifacts, including:\n",
    "   * Story intent (logline, conflict, emotional arc)\n",
    "   * Scene and beat segmentation\n",
    "   * Dialogue grammar and audio continuity\n",
    "   * Spatial and character continuity\n",
    "   * Performance timing and cut motivation\n",
    "3. Editorial Logic Stage:\n",
    "   These artifacts are validated and reconciled using deterministic editorial rules to ensure narrative coherence, continuity, and pacing.\n",
    "4. Edit Generation Stage:\n",
    "   Based on the validated understanding, the system generates edit instructions (subclips, transitions, timing) rather than rendering final creative decisions.\n",
    "5. Export Stage:\n",
    "   The final output is an **XML timeline** compatible with professional editing software (Adobe Premiere Pro), allowing human editors to refine and finalize the edit.\n",
    "\n",
    "### Justification of Design Choices:\n",
    "* LLM-centric reasoning was chosen to model high-level editorial judgment that cannot be captured through traditional feature-based ML.\n",
    "* Artifact-based understanding enables transparency, inspection, and human correction before edits are generated.\n",
    "* Local processing for media files ensures scalability without requiring high-cost cloud infrastructure.\n",
    "* Separation of understanding and execution aligns with professional editing workflows and avoids treating AI output as final creative authority.\n",
    "\n",
    "This design prioritizes editorial correctness, interpretability, and human-in-the-loop control, making it suitable for real-world film editing rather than purely automated video assembly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ba9fca-118e-4c51-8569-373de63f78ef",
   "metadata": {},
   "source": [
    "### CORE IMPLEMENTATION OVERVIEW:\n",
    "\n",
    "### Model Training / Inference Logic\n",
    "EditFlows AI does not involve model training. All AI components operate in inference-only mode using pre-trained, production-grade models accessed via APIs.\n",
    "The system orchestrates multiple inference calls to:\n",
    "\n",
    "* Transcribe speech\n",
    "* Interpret visual context at a high level\n",
    "* Perform narrative and editorial reasoning\n",
    "\n",
    "Inference outputs are converted into structured editorial artifacts (JSON schemas) rather than direct video edits, ensuring determinism and auditability.\n",
    "\n",
    "### Prompt Engineering (LLM-Based System)\n",
    "Prompt engineering is a central implementation component of EditFlows AI.\n",
    "LLMs are guided through:\n",
    "\n",
    "* Explicit editorial roles (e.g., narrative editor, emotion-driven editor)\n",
    "* Constrained reasoning instructions aligned with film grammar\n",
    "* Deterministic configurations (temperature = 0)\n",
    "* Schema-enforced outputs to prevent hallucination\n",
    "\n",
    "Prompts are modular and stage-specific, enabling step-by-step reasoning for story intent, continuity analysis, cut motivation, and timing decisions.\n",
    "\n",
    "### Recommendation / Prediction Pipeline\n",
    "Instead of predicting outcomes, the system implements a rule-constrained recommendation pipeline that suggests:\n",
    "* Scene ordering\n",
    "* Shot selection\n",
    "* Subclip boundaries\n",
    "* Transition requirements\n",
    "* Timing and pacing adjustments\n",
    "\n",
    "These recommendations are derived from validated understanding artifacts and editorial logic, not statistical optimization. Final creative decisions remain with the human editor through exported timelines.\n",
    "\n",
    "This implementation emphasizes interpretability, editorial correctness, and professional workflow compatibility, distinguishing it from end-to-end automated video generation systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e31128-20ec-4ad9-9bd5-44a6dfec803d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b3b1e9f-ee4c-4ba5-9186-0147f8156ffb",
   "metadata": {},
   "source": [
    "### NEW PROJECT IMPLEMENTATION - STEP-BY-STEP PROCESS:\n",
    "\n",
    "## Core Implementation\n",
    "EditFlows AI is implemented as a **local-first, inference-driven AI editing system** that transforms raw footage into structured editorial decisions and professional editing timelines. The implementation combines deterministic LLM reasoning, rule-based media processing, and industry-standard export formats.\n",
    "\n",
    "### 1. Model Training / Inference Logic\n",
    "EditFlows AI **does not train any machine learning models**.\n",
    "All intelligence is derived from **pre-trained foundation models**, used strictly in inference mode via APIs.\n",
    "\n",
    "Inference responsibilities include:\n",
    "* Speech transcription (dialogue extraction)\n",
    "* Visual understanding (high-level scene interpretation)\n",
    "* Editorial reasoning (story flow, continuity, pacing, cut motivation)\n",
    "\n",
    "All inference outputs are converted into **structured JSON artifacts** rather than directly modifying media, ensuring transparency, auditability, and editorial control.\n",
    "\n",
    "### 2. Prompt Engineering (LLM-Based System)\n",
    "Prompt engineering is a **core implementation layer** rather than a peripheral feature.\n",
    "Key characteristics:\n",
    "* Explicit editorial roles (Narrative, Emotion, Rhythm personas)\n",
    "* Deterministic execution (temperature = 0)\n",
    "* Schema-constrained outputs (JSON only)\n",
    "* Editorial grammar constraints (dialogue logic, spatial continuity, pacing rules)\n",
    "\n",
    "Each LLM interaction is scoped to a specific editorial task (story intent interpretation, scene breakdown, cut validation), preventing uncontrolled generation and preserving professional editing standards.\n",
    "\n",
    "### 3. Recommendation / Decision Pipeline\n",
    "EditFlows AI does **not predict outcomes**; instead, it generates **rule-constrained editorial recommendations**, including:\n",
    "\n",
    "* Scene ordering\n",
    "* Shot selection\n",
    "* Subclip boundaries\n",
    "* Transition requirements\n",
    "* Performance timing adjustments\n",
    "\n",
    "Final creative authority remains with the human editor through reviewable artifacts and exported timelines.\n",
    "\n",
    "## EditFlows AI ‚Äî New Project Workflow\n",
    "This section describes the **complete end-to-end workflow**, from raw footage ingestion to Adobe Premiere Pro XML export.\n",
    "\n",
    "### Prerequisites\n",
    "| Requirement       | Notes                                                            |\n",
    "| ----------------- | ---------------------------------------------------------------- |\n",
    "| FFmpeg            | Must be installed and available in system PATH                   |\n",
    "| Python 3.10+      | Required packages: anthropic, openai, moviepy, flask, flask-cors |\n",
    "| ANTHROPIC_API_KEY | Environment variable for Claude AI                               |\n",
    "| OPENAI_API_KEY    | Environment variable for Whisper + GPT-4o                        |\n",
    "| Raw Footage       | MP4 / MOV / MXF files (maximum 50 clips)                         |\n",
    "\n",
    "\n",
    "## Phase 1: Media Preparation\n",
    "### Step 1.1 ‚Äî Organize Raw Footage\n",
    "Place all raw clips in a single directory:\n",
    "```\n",
    "D:\\YourProject\\RawFootage\\\n",
    "‚îú‚îÄ‚îÄ video_001.mp4\n",
    "‚îú‚îÄ‚îÄ video_002.mp4\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "### Step 1.2 ‚Äî Update Path Configuration\n",
    "Edit the following file:\n",
    "\n",
    "```\n",
    "src/transcode.py   (lines 609‚Äì610)\n",
    "```\n",
    "\n",
    "```python\n",
    "INPUT_DIR = r\"D:\\YourProject\\RawFootage\"\n",
    "OUTPUT_BASE = r\"d:\\Srinivas_SDE\\EditFlows AI v0.1\"\n",
    "```\n",
    "\n",
    "### Step 1.3 ‚Äî Run Transcoding\n",
    "```bash\n",
    "cd \"d:\\Srinivas_SDE\\EditFlows AI v0.1\"\n",
    "python src/transcode.py\n",
    "```\n",
    "\n",
    "**Generated outputs:**\n",
    "* `media/editorial/*.mov` ‚Üí ProRes 422 (editing media)\n",
    "* `media/preview/*.mp4` ‚Üí H.264 (UI previews)\n",
    "* `media/media_map.json` ‚Üí Media registry\n",
    "\n",
    "\n",
    "## Phase 2: Audio & Content Analysis\n",
    "### Step 2.1 ‚Äî Update Ingest Paths\n",
    "\n",
    "Edit:\n",
    "```\n",
    "src/ingest.py   (lines 171‚Äì172)\n",
    "```\n",
    "\n",
    "```python\n",
    "PROCESSED = r\"d:\\Srinivas_SDE\\EditFlows AI v0.1\\data\\processed\"\n",
    "RAW = r\"D:\\YourProject\\RawFootage\"\n",
    "```\n",
    "\n",
    "### Step 2.2 ‚Äî Run Ingestion\n",
    "```bash\n",
    "python src/ingest.py\n",
    "```\n",
    "\n",
    "**Outputs:**\n",
    "* `data/processed/audio/*.wav` ‚Üí Extracted audio\n",
    "* `data/processed/clips_registry.json` ‚Üí Initial clip metadata\n",
    "\n",
    "### Step 2.3 ‚Äî Run AI Analysis\n",
    "\n",
    "```bash\n",
    "python src/analyze.py\n",
    "```\n",
    "Enhances `clips_registry.json` with:\n",
    "* Whisper transcripts\n",
    "* GPT-4o visual descriptions\n",
    "* Dialogue density classification\n",
    "* Speech profiling (in-scene vs production audio)\n",
    "\n",
    "## Phase 3: Story Intent (Manual Editorial Input)\n",
    "\n",
    "### Step 3.1 ‚Äî Create Story Intent File\n",
    "Create or edit:\n",
    "```\n",
    "data/processed/story_intent.json\n",
    "```\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"story_title\": \"Your Film Title\",\n",
    "  \"story_summary\": \"One paragraph describing the story...\",\n",
    "  \"emotional_tone\": \"Overall mood of the film\",\n",
    "  \"narrative_arc\": \"Beginning ‚Üí Middle ‚Üí Climax ‚Üí Resolution\",\n",
    "  \"character_journey\": \"Protagonist‚Äôs emotional and narrative journey\",\n",
    "  \"editorial_notes\": \"Key editing guidance for AI\",\n",
    "  \"constraints\": [\n",
    "    \"Single location\",\n",
    "    \"Exclude promotional content\"\n",
    "  ],\n",
    "  \"central_conflict\": \"Core dramatic tension\",\n",
    "  \"logline\": \"One-sentence story summary\"\n",
    "}\n",
    "```\n",
    "**Important:**\n",
    "This file is **mandatory and manual**.\n",
    "All AI editorial decisions are grounded in this document.\n",
    "\n",
    "## Phase 4: Generate Edits (UI)\n",
    "\n",
    "### Step 4.1 ‚Äî Start Backend Server\n",
    "```bash\n",
    "python src/api_server.py\n",
    "```\n",
    "Server runs at:\n",
    "`http://localhost:5000`\n",
    "---\n",
    "\n",
    "### Step 4.2 ‚Äî Start Frontend\n",
    "```bash\n",
    "cd ui_prototype\n",
    "npm run dev\n",
    "```\n",
    "UI available at:\n",
    "`http://localhost:5173` (or 5174)\n",
    "\n",
    "### Step 4.3 ‚Äî Generate Edits\n",
    "* Navigate to **Edits** tab\n",
    "* Customize persona prompts (Narrative / Emotion / Rhythm)\n",
    "* Click **Generate Edit**\n",
    "* Wait for AI reasoning + MoviePy assembly (‚âà 1‚Äì3 minutes per persona)\n",
    "\n",
    "## Phase 5: Export to Adobe Premiere Pro\n",
    "\n",
    "### Step 5.1 ‚Äî Export XML\n",
    "* Go to **Export** tab\n",
    "* Click **Export XML** for desired persona\n",
    "\n",
    "**Output location:**\n",
    "```\n",
    "data/processed/premiere_export/\n",
    "```\n",
    "### Step 5.2 ‚Äî Import into Premiere Pro\n",
    "1. Open Adobe Premiere Pro\n",
    "2. File ‚Üí Import\n",
    "3. Select the exported `.xml` file\n",
    "\n",
    "The timeline references **ProRes 422 media** from:\n",
    "\n",
    "```\n",
    "media/editorial/\n",
    "```\n",
    "## Quick Reference: Key Files\n",
    "\n",
    "| File                    | Purpose                     |\n",
    "| ----------------------- | --------------------------- |\n",
    "| `media/media_map.json`  | Master media registry       |\n",
    "| `clips_registry.json`   | Clip metadata + transcripts |\n",
    "| `story_intent.json`     | Manual editorial direction  |\n",
    "| `edit_flows.json`       | Generated edit sequences    |\n",
    "| `premiere_export/*.xml` | Premiere Pro timelines      |\n",
    "\n",
    "## Python Scripts developed in the project:\n",
    "\n",
    "### Root Level (`/`)\n",
    "* Dummy data generator (from create_dummy_data.py)\n",
    "\n",
    "### Source (`src/`)\n",
    "* Media analysis pipeline (from analyze.py)\n",
    "* API server for UI backend (from api_server.py)\n",
    "* Editorial logic processing (from editorial_logic.py)\n",
    "* Media ingestion pipeline (from ingest.py)\n",
    "* Premiere Pro exporter (from premiere_exporter.py)\n",
    "* Preview video generator (from preview_generator.py)\n",
    "* Media map refresh utility (from refresh_media_map.py)\n",
    "* Spatial continuity analyzer (from spatial_analyzer.py)\n",
    "* Media transcoding pipeline (from transcode.py)\n",
    "* XML export functionality (from xml_exporter.py)\n",
    "\n",
    "### Summary\n",
    "This implementation prioritizes:\n",
    "* Editorial correctness over automation\n",
    "* Human-in-the-loop control\n",
    "* Professional post-production compatibility\n",
    "* Deterministic, inspectable AI reasoning\n",
    "\n",
    "It establishes EditFlows AI as a **practical AI-assisted editing system**, not a black-box generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3805f90-78d2-498b-bb7b-ff30a37ec2ff",
   "metadata": {},
   "source": [
    "### PROJECT STRUCTURE:\n",
    "\n",
    "# EditFlows AI - Project Structure\n",
    "EditFlows AI is an intelligent video editing assistant that analyzes media content and generates AI-powered edit recommendations for professional video editors.\n",
    "\n",
    "## üìÅ Directory Overview\n",
    "\n",
    "EditFlows AI v0.1/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ src/                   # Python backend scripts\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Original input data\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ processed/         # AI-generated analysis outputs\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ media/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Raw video files\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ editorial/         # Editorial cut files\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ preview/           # Preview thumbnails\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ ui_prototype/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ src/               # Frontend source code\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ ui_designs/            # UI design assets\n",
    "\n",
    "## üêç Python Scripts\n",
    "\n",
    "### Root Level (`/`)\n",
    "* Dummy data generator (from create_dummy_data.py)\n",
    "\n",
    "### Source (`src/`)\n",
    "* Media analysis pipeline (from analyze.py)\n",
    "* API server for UI backend (from api_server.py)\n",
    "* Editorial logic processing (from editorial_logic.py)\n",
    "* Media ingestion pipeline (from ingest.py)\n",
    "* Premiere Pro exporter (from premiere_exporter.py)\n",
    "* Preview video generator (from preview_generator.py)\n",
    "* Media map refresh utility (from refresh_media_map.py)\n",
    "* Spatial continuity analyzer (from spatial_analyzer.py)\n",
    "* Media transcoding pipeline (from transcode.py)\n",
    "* XML export functionality (from xml_exporter.py)\n",
    "\n",
    "## üìä JSON Data Objects\n",
    "\n",
    "### Structured Validation Checks (`data/processed/`)\n",
    "* Cut motivation validation (from `cut_validation.json`)\n",
    "* Audio continuity validation (from `audio_continuity.json`)\n",
    "* Clips registry (from `clips_registry.json`)\n",
    "* Dialogue grammar rules (from `dialogue_grammar.json`)\n",
    "* Edit flow definitions (from `edit_flows.json`)\n",
    "* Performance timing data (from `performance_timing.json`)\n",
    "* Scene definitions (from `scenes.json`)\n",
    "* Spatial continuity validation (from `spatial_continuity.json`)\n",
    "* Story intent data (from `story_intent.json`)\n",
    "* Take grouping data (from `take_groups.json`)\n",
    "* Transition definitions (from `transitions.json`)\n",
    "\n",
    "### Media & Schema (`media/`, `/`)\n",
    "* Media file mapping (from `media_map.json`)\n",
    "* Scene order schema definition (from `scene_order_schema.json`)\n",
    "\n",
    "## ‚öõÔ∏è React Frontend (`ui_prototype/src/`)\n",
    "### Core Application\n",
    "* Main application entry (from `main.tsx`)\n",
    "* App router and layout (from `App.tsx`)\n",
    "* Type definitions (from `types.ts`)\n",
    "\n",
    "### Pages (`pages/`)\n",
    "* Project overview page (from `ProjectPage.tsx`)\n",
    "* Media browser page (from `MediaPage.tsx`)\n",
    "* AI understanding dashboard (from `AIUnderstandingPage.tsx`)\n",
    "* Edit generation page (from `EditPage.tsx`)\n",
    "* Export workflow page (from `ExportPage.tsx`)\n",
    "\n",
    "### Components (`components/`)\n",
    "#### Layout Components (`components/layout/`)\n",
    "* Inspector panel (from `InspectorPanel.tsx`)\n",
    "* Page layout wrapper (from `PageLayout.tsx`)\n",
    "\n",
    "#### Common Components (`components/common/`)\n",
    "* Inline edit field (from `InlineEditField.tsx`)\n",
    "* Inline edit text area (from `InlineEditTextArea.tsx`)\n",
    "* Multi-line editable field (from `MultiLineEditableField.tsx`)\n",
    "* Phase progress bar (from `PhaseBar.tsx`)\n",
    "* Status bar (from `StatusBar.tsx`)\n",
    "* Title bar (from `TitleBar.tsx`)\n",
    "\n",
    "#### AI Understanding Panels (`components/aiUnderstanding/`)\n",
    "**Story**\n",
    "* Central conflict panel (from `CentralConflictPanel.tsx`)\n",
    "* Story overview panel (from `StoryOverviewPanel.tsx`)\n",
    "\n",
    "**Visual**\n",
    "* Scene order panel (from `SceneOrderPanel.tsx`)\n",
    "\n",
    "### Styles (`src/`)\n",
    "* Global styles (from `index.css`)\n",
    "* App-specific styles (from `App.css`)\n",
    "\n",
    "## üì¶ Configuration Files\n",
    "### Root Level (`/`)\n",
    "* Python dependencies (from `requirements.txt`)\n",
    "\n",
    "### UI Prototype (`ui_prototype/`)\n",
    "* Node.js dependencies (from `package.json`)\n",
    "* Vite configuration (from `vite.config.ts`)\n",
    "* TypeScript configuration (from `tsconfig.json`, `tsconfig.app.json`, `tsconfig.node.json`)\n",
    "* ESLint configuration (from `eslint.config.js`)\n",
    "* Git ignore rules (from `.gitignore`)\n",
    "* Project documentation (from `README.md`)\n",
    "\n",
    "## üîÑ Data Flow\n",
    "Raw Media ‚Üí Ingestion ‚Üí Analysis ‚Üí AI Processing ‚Üí Edit Recommendations ‚Üí Premiere Export ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì ‚Üì media/raw ingest.py analyze.py api_server.py edit_flows.json xml_exporter.py\n",
    "\n",
    "## üöÄ EditFlows AI - Quick Start:\n",
    "\n",
    "1. **Install Python dependencies:**\n",
    "   ```bash\n",
    "   pip install -r requirements.txt\n",
    "\n",
    "2. **Install Node.js dependencies**\n",
    "   cd ui_prototype\n",
    "   npm install\n",
    "\n",
    "3. **Start the backend API server**\n",
    "   python src/api_server.py\n",
    "\n",
    "4. **Start the frontend development server**\n",
    "   cd ui_prototype\n",
    "   npm run dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ac147-40e9-4621-b7e0-4c80c001befe",
   "metadata": {},
   "source": [
    "### EVALUATION & ANALYSIS:\n",
    "\n",
    "### 1. Evaluation Metrics\n",
    "Since EditFlows AI is an **editorial intelligence system** rather than a predictive model, evaluation is primarily **qualitative**, supported by structured validation artifacts.\n",
    "\n",
    "## Qualitative Metrics\n",
    "* **Narrative Coherence**:\n",
    "  Whether the generated edit preserves a clear beginning, escalation, turning point, and resolution.\n",
    "* **Continuity Integrity**:\n",
    "  Absence of spatial, visual, and dialogue continuity errors across cuts.\n",
    "* **Editorial Motivation per Cut**:\n",
    "  Each cut must be justifiable by at least one editorial reason (information change, emotional shift, spatial change, pacing).\n",
    "* **Human Editorial Acceptability**:\n",
    "  Degree to which a professional editor can accept the AI-generated cut as a valid first assembly.\n",
    "\n",
    "## Structured Validation Checks\n",
    "### Data Processing (`data/processed/`)\n",
    "* Cut motivation validation (from `cut_validation.json`)\n",
    "* Audio continuity validation (from `audio_continuity.json`)\n",
    "* Clips registry (from `clips_registry.json`)\n",
    "* Dialogue grammar rules (from `dialogue_grammar.json`)\n",
    "* Edit flow definitions (from `edit_flows.json`)\n",
    "* Performance timing data (from `performance_timing.json`)\n",
    "* Scene definitions (from `scenes.json`)\n",
    "* Spatial continuity validation (from `spatial_continuity.json`)\n",
    "* Story intent data (from `story_intent.json`)\n",
    "* Take grouping data (from `take_groups.json`)\n",
    "* Transition definitions (from `transitions.json`)\n",
    "\n",
    "### Media & Schema\n",
    "* Media file mapping (from `media_map.json`)\n",
    "* Scene order schema definition (from `scene_order_schema.json`)\n",
    "\n",
    "### 2. Sample Outputs\n",
    "The system produces multiple inspectable artifacts rather than a single opaque output:\n",
    "\n",
    "* **Editorial Understanding Artifacts**\n",
    "\n",
    "  * Story intent (`story_intent.json`)\n",
    "  * Scene definitions and emotional states (`scenes.json`)\n",
    "  * Dialogue grammar analysis (`dialogue_grammar.json`)\n",
    "  * Spatial continuity maps (`spatial_continuity.json`)\n",
    "  * Performance timing rules (`performance_timing.json`)\n",
    "\n",
    "* **Generated Edit Outputs**\n",
    "  * Persona-based edit flows (Narrative / Emotion / Rhythm)\n",
    "  * Preview renders (H.264)\n",
    "  * Adobe Premiere Pro XML timelines referencing ProRes 422 media\n",
    "\n",
    "Sample results demonstrate:\n",
    "* Correct ordering of scenes according to narrative intent\n",
    "* Preservation of emotional beats through timing adjustments\n",
    "* Removal of production audio while retaining intentional silence\n",
    "* Spatially coherent transitions between locations\n",
    "\n",
    "### 3. Performance Analysis\n",
    "**System Performance**\n",
    "* Average AI analysis time per clip: 5‚Äì15 seconds (API-dependent)\n",
    "* Edit generation per persona: ~1‚Äì3 minutes\n",
    "* XML export time: <5 seconds\n",
    "\n",
    "**Strengths**\n",
    "* Deterministic and repeatable outputs\n",
    "* High transparency through intermediate artifacts\n",
    "* Meaningful human-in-the-loop correction before final export\n",
    "* Direct compatibility with professional editing software\n",
    "\n",
    "### 4. Limitations\n",
    "* **No Real-Time Processing**:\n",
    "  Designed for offline editing workflows, not live editing.\n",
    "* **Dependency on API Availability**:\n",
    "  Performance and cost depend on external LLM services.\n",
    "* **No Automatic Creative Judgment**:\n",
    "  Final creative quality still depends on human-authored story intent.\n",
    "* **Single-Project Focus (MVP)**:\n",
    "  Edit history and version tracking are not maintained.\n",
    "\n",
    "### Summary\n",
    "The evaluation demonstrates that EditFlows AI is effective as a **decision-support system for film editing**, producing editorially coherent first cuts while preserving human creative authority. Rather than optimizing numerical accuracy, the system is validated on **story clarity, continuity correctness, and professional usability**, which are the true success metrics in cinematic post-production.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd241ef-d60c-4902-a29c-8ef5998de524",
   "metadata": {},
   "source": [
    "### ETHICAL CONSIDERATIONS & RESPONSIBLE AI:\n",
    "\n",
    "### 1. Bias and Fairness Considerations\n",
    "EditFlows AI does not make autonomous creative or moral decisions. All editorial outputs are **guided by human-authored story intent and editorial constraints**, reducing the risk of unintended narrative or representational bias.\n",
    "\n",
    "Potential biases inherited from large language and vision models (e.g., cultural assumptions in scene interpretation) are mitigated by:\n",
    "\n",
    "* Keeping editorial understanding **fully visible and editable** to the user.\n",
    "* Allowing human correction before any edit is generated.\n",
    "* Preventing automatic final exports without explicit human approval.\n",
    "\n",
    "Thus, creative authority and responsibility remain with the editor.\n",
    "\n",
    "### 2. Dataset Limitations\n",
    "The system does not rely on a fixed or curated training dataset. Instead:\n",
    "\n",
    "* It processes **user-provided footage only**.\n",
    "* It leverages pre-trained third-party models (LLMs, speech-to-text, vision models) in an inference-only capacity.\n",
    "* No footage is used to train or fine-tune models within the system.\n",
    "\n",
    "As a result, any limitations stem from:\n",
    "\n",
    "* The quality and clarity of the input footage.\n",
    "* The inherent constraints of third-party AI services.\n",
    "\n",
    "### 3. Responsible Use of AI Tools\n",
    "EditFlows AI is designed as a **human-in-the-loop editorial assistant**, not an autonomous editor.\n",
    "\n",
    "Responsible AI practices embedded in the system include:\n",
    "* Explicit separation between AI understanding and AI action.\n",
    "* Mandatory human review points before preview rendering and XML export.\n",
    "* Clear labeling of AI-generated interpretations versus human-authored inputs.\n",
    "* Local-first media handling to reduce unnecessary data exposure.\n",
    "\n",
    "The system emphasizes **augmentation over replacement**, supporting professional editors rather than attempting to automate creative authorship.\n",
    "\n",
    "### Ethics Compliance Statement\n",
    "EditFlows AI follows responsible AI principles by ensuring transparency, human control, limited automation, and ethical use of third-party AI services. The system prioritizes creative accountability, data privacy, and editorial integrity, making it suitable for professional and academic contexts.\n",
    "\n",
    "This project complies with ethical and responsible AI practices by ensuring that all creative and editorial decisions remain under human control. EditFlows AI operates exclusively as a decision-support system, using pre-trained AI models for inference without training on user data. All media processing is performed on user-provided content, with full transparency and opportunities for human review, correction, and approval before final outputs are generated. No personal data is collected, stored, or reused beyond the scope of the active editing session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfe350-3c71-462e-b91c-bcdf085c1072",
   "metadata": {},
   "source": [
    "### CONCLUSION & FUTURE SCOPE:\n",
    "\n",
    "### Conclusion\n",
    "This project demonstrated that meaningful film editing intelligence can be achieved without training models from scratch by effectively orchestrating existing large language models and AI tools within a structured editorial pipeline. \n",
    "\n",
    "EditFlows AI successfully generated multiple editorially distinct first-cut edits from raw footage by combining story intent, audio‚Äìvisual understanding, continuity reasoning, and deterministic edit logic. \n",
    "\n",
    "The system proved that AI can assist editors not merely by assembling clips, but by reasoning about narrative flow, emotional progression, spatial continuity, and dialogue structure‚Äîwhile preserving full human control over final creative decisions.\n",
    "\n",
    "### Future Scope\n",
    "\n",
    "* Going forward, this system can be extended with deeper AI Understanding modules and tested on longer-form films.\n",
    "* There is also potential for real-world editorial use and further research into AI-assisted creativity rather than automation.\n",
    "\n",
    "Additional extensions could support larger-scale productions, collaborative multi-editor workflows, advanced continuity validation, and tighter integration with professional post-production tools beyond Premiere Pro. \n",
    "\n",
    "In the long term, EditFlows AI can evolve into a comprehensive AI Film Studio platform, enabling intelligent assistance across scripting, editing, sound design, and narrative validation‚Äîwhile maintaining ethical, human-centered creative control.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f269a-cb78-4df7-88fe-03bd6ffe1abf",
   "metadata": {},
   "source": [
    "### ACKNOWLEDGEMENTS\n",
    "\n",
    "### THANK YOU ALL\n",
    "\n",
    "**IIT Ropar**\n",
    "I sincerely thank the management and faculty of IIT Ropar for initiating and delivering this well-structured and impactful program on Artificial Intelligence and Machine Learning, which provided strong academic grounding and inspiration for this project.\n",
    "\n",
    "**Kartik Gupta** ([LinkedIn](https://www.linkedin.com/in/kartikgupta98/))\n",
    "I would like to express my heartfelt gratitude to Kartik Gupta for his guidance and mentorship throughout this project, from initial ideation to final submission.\n",
    "\n",
    "**Masai**\n",
    "I extend my sincere thanks to Masai for efficiently managing and delivering this year-long program, ensuring a smooth learning experience through consistent support, coordination, and execution.\n",
    "\n",
    "Finally, I am deeply grateful to everyone who supported and encouraged me throughout this one-year journey of completing the *Minor in AI* program from **IIT Ropar in collaboration with Masai**. This project stands as a culmination of that learning experience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eaa81e-90f6-4132-abbb-f802e73d334a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
